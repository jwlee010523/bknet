# CODE
* 코드 안에서 데브셋 나누기
* API 통일하기 (config, wx, wx_num, cx, cx_num, sx, sx_num, is_training)

##### RNN
* hierarchical attention network (HAN)
* densely connected bidirectional lstm with applications to sentence classification

##### CNN

* do convolutional networks need to be deep for text classification?
* very deep convolution network for text classification
* convolutional neural networks for sentence classification
* TCN https://arxiv.org/pdf/1803.01271.pdf

##### Attention

* QANet: combining local convolution with global self-attention for reading comprehension
* a structured self-attentive sentence embedding

##### Input

* pre-trained fast-text 붙이기 300짜리
* word embedding matrix (<= trainable) tf.assign
* knn
* fast-text
* byte-encoding
* ElMo


* word embedding matrix (<= trainable) tf.assign
* TCN https://arxiv.org/pdf/1803.01271.pdf
* 틀린 케이스 뽑는 코드